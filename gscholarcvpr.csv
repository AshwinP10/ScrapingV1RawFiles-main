Document Title,Authors,Publication Year,PDF Link,Implementation?,Abstract Summary
Classifying Group Emotions for Socially-Aware Autonomous Vehicle Navigation,A. Bera; T. Randhavane; A. Wang; D. Manocha; E. Kubin; K. Gray,2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575305,NO,"Our approach is based on prior psychological research, which reveals that people notice and-importantly-react negatively to groups of social actors when they have negative group emotions or entitativity, moving in a tight group with similar appearances and trajectories. Our approach empirically shows that various levels of emotional autonomous robots can be used to both avoid and influence pedestrians while not eliciting strong emotional reactions, giving multi-robot systems socially-invisibility."
JRDB-Pose: A Large-Scale Dataset for Multi-Person Pose Estimation and Tracking,E. Vendrow; D. T. Le; J. Cai; H. Rezatofighi,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204595,NO,"JRDB-Pose extends the existing JRDB which includes videos captured from a social navigation robot in a university campus environment, containing challenging scenes with crowded indoor and outdoor locations and a diverse range of scales and occlusion types. In crowded human scenes with close-up human-robot interaction and robot navigation, a deep understanding of surrounding people requires reasoning about human motion and body dynamics over time with human body pose estimation and tracking."
Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments,S. Dong; Q. Fan; H. Wang; J. Shi; L. Yi; T. Funkhouser; B. Chen; L. Guibas,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577932,NO,"It builds on three important blocks: (a) a hierarchical space partition over the indoor scene to construct the decision tree; (b) a neural routing function, implemented as a deep classification network, employed for better 3D scene understanding; and (c) an outlier rejection module used to filter out dynamic points during the hierarchical routing process. Such a mapping is estimated with either a convolution neural network or a decision tree using only the static input image sequence, which makes these approaches vulnerable to dynamic indoor environments that are quite common yet challenging in the real world."
Multi-Agent Tensor Fusion for Contextual Trajectory Prediction,T. Zhao; Y. Xu; M. Monfort; W. Choi; C. Baker; Y. Zhao; Y. Wang; Y. N. Wu,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953520,NO,"Specifically, the model encodes multiple agents' past trajectories and the scene context into a Multi-Agent Tensor, then applies convolutional fusion to capture multiagent interactions while retaining the spatial structure of agents and the scene context. Trajectory prediction is challenging because it requires reasoning about agents' past movements, social interactions among varying numbers and kinds of agents, constraints from the scene context, and the stochasticity of human behavior."
Boosting Detection in Crowd Analysis via Underutilized Output Features,S. Wu; F. Yang,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203588,YES,"Our extensive evaluations on various crowd analysis tasks, including crowd counting, localization, and detection, demonstrate the effectiveness of utilizing output features and the potential of detection-based methods in crowd analysis. However, we argue that the potential of these methods has been underestimated, as they offer crucial information for crowd analysis that is often ignored."
Tracking Pedestrian Heads in Dense Crowd,R. Sundararaman; C. De Almeida Braga; E. Marchand; J. Pettré,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577483,NO,"For evaluation, we proposed a new metric, IDEucl, to measure an algorithm’s efficacy in preserving a unique identity for the longest stretch in image coordinate space, thus building a correspondence between pedestrian crowd motion and the performance of a tracking algorithm. For that reason, we propose to revitalize head tracking with Crowd of Heads Dataset (CroHD), consisting of 9 sequences of 11,463 frames with over 2,276,838 heads and 5,230 tracks annotated in diverse scenes."
StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For Multi-Agent Environments,S. Kulinski; N. R. Waytowich; J. Z. Hare; D. I. Inouye,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204010,NO,"Spatial reasoning tasks in multi-agent environments such as event prediction, agent type identification, or missing data imputation are important for multiple applications (e.g., autonomous surveillance over sensor networks and subtasks for reinforcement learning (RL)). Following the simplicity of these datasets, we construct a benchmark spatial reasoning dataset based on StarCraft II replays that exhibit complex multi-agent behaviors, while still being as easy to use as MNIST and CIFAR10."
Progressive End-to-End Object Detection in Crowded Scenes,A. Zheng; Y. Zhang; X. Zhang; X. Qi; J. Sun,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878585,YES,"Previous query-based detectors suffer from two drawbacks: first, multiple predictions will be inferred for a single object, typically in crowded scenes; second, the performance saturates as the depth of the decoding stage increases. Equipped with our approach, Sparse RCNN achieves 92.0% AP, 41.4% MR−2 and 83.2% JI on the challenging CrowdHuman [35] dataset, outperforming the box-based method MIP [8] that specifies in handling crowded scenarios."
CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model,D. Liang; J. Xie; Z. Zou; X. Ye; W. Xu; X. Bai,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203327,YES,"In the testing stage, to deal with the diversity of image patches, we propose a simple yet effective progressive filtering strategy to first select the highly potential crowd patches and then map them into the language space with various counting intervals. The core idea is built on two observations: 1) the recent contrastive pre-trained vision-language model (CLIP) has presented impressive performance on various downstream tasks; 2) there is a natural mapping between crowd patches and count text."
IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction,D. Zhu; G. Zhai; Y. Di; F. Manhardt; H. Berkemeyer; T. Tran; N. Navab; F. Tombari; B. Busam,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203793,NO,"Compared with single-agent cases, the major challenge in simultaneously processing multiple agents lies in modeling complex social interactions caused by various driving intentions and road conditions. Previous methods typically leverage graph-based message propagation or attention mechanism to encapsulate such interactions in the format of marginal probabilistic distributions."
FJMP: Factorized Joint Multi-Agent Motion Prediction over Learned Directed Acyclic Interaction Graphs,L. Rowe; M. Ethier; E. -H. Dykhne; K. Czarnecki,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204178,NO,"We then prune the graph into a directed acyclic graph (DAG) and decompose the joint prediction task into a sequence of marginal and conditional predictions according to the partial ordering of the DAG, where joint future trajectories are decoded using a directed acyclic graph neural network (DAGNN). We conduct experiments on the INTERACTION and Argoverse 2 datasets and demonstrate that FJMP produces more accurate and scene-consistent joint trajectory predictions than non-factorized approaches, especially on the most interactive and kinematically interesting agents."
Context-Aware Crowd Counting,W. Liu; M. Salzmann; P. Fua,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954153,NO,"In this paper, we introduce an end-to-end trainable deep architecture that combines features obtained using multiple receptive field sizes and learns the importance of each such feature at each image location. This is typically achieved by training an auxiliary classifier to select, for predefined image patches, the best kernel size among a limited set of choices."
A Generalized Loss Function for Crowd Counting and Localization,J. Wan; Z. Liu; A. B. Chan,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578673,NO,"In this paper, we investigate learning the density map representation through an unbalanced optimal transport problem, and propose a generalized loss function to learn density maps for crowd counting and localization. Since the predicted density will be pushed toward annotation positions, the density map prediction will be sparse and can naturally be used for localization."
Leveraging Self-Supervision for Cross-Domain Crowd Counting,W. Liu; N. Durasov; P. Fua,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878515,YES,"To this end, we force our network to learn perspective-aware features by training it to recognize upside-down real images from regular ones and incorporate into it the ability to predict its own uncertainty so that it can generate useful pseudo labels for fine-tuning purposes. While effective, these data-driven approaches rely on large amount of data annotation to achieve good performance, which stops these models from being deployed in emergencies during which data annotation is either too costly or cannot be obtained fast enough."
Revisiting Perspective Information for Efficient Crowd Counting,M. Shi; Z. Yang; C. Xu; Q. Chen,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953508,NO,"Ground truth perspective maps are firstly generated for training; PACNN is then specifically designed to predict multi-scale perspective maps and encode them as perspective-aware weighting layers in the network to adaptively combine the outputs of multi-scale density maps. In this work, we propose a perspective-aware convolutional neural network (PACNN) for efficient crowd counting, which integrates the perspective information into density regression to provide additional knowledge of the person scale change in an image."
Cross-Modal Collaborative Representation Learning and a Large-Scale RGBT Benchmark for Crowd Counting,L. Liu; J. Chen; H. Wu; G. Li; C. Li; L. Lin,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578312,NO,"Furthermore, to facilitate the multimodal crowd counting, we propose a cross-modal collaborative representation learning framework, which consists of multiple modality-specific branches, a modality-shared branch, and an Information Aggregation-Distribution Module (IADM) to capture the complementary information of different modalities fully. To promote future researches in this field, we introduce a large-scale RGBT Crowd Counting (RGBT-CC) benchmark, which contains 2,030 pairs of RGB-thermal images with 138,389 annotated people."
Human Trajectory Prediction with Momentary Observation,J. Sun; Y. Li; L. Chai; H. -S. Fang; Y. -L. Li; C. Lu,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878712,NO,"In this paper, we study a task named momentary trajectory prediction, which reduces the observed history from a long time sequence to an extreme situation of two frames, one frame for social and scene contexts and both frames for the velocity of agents. Human trajectory prediction task aims to analyze human future movements given their past status, which is a crucial step for many autonomous systems such as self-driving cars and social robots."
Learning From Synthetic Data for Crowd Counting in the Wild,Q. Wang; J. Gao; W. Lin; Y. Yuan,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953868,YES,"Secondly, we propose two schemes that exploit the synthetic data to boost the performance of crowd counting in the wild: 1) pretrain a crowd counter on the synthetic data, then finetune it using the real data, which significantly prompts the model's performance on real data; 2) propose a crowd counting method via domain adaptation, which can free humans from heavy data annotations. To remedy the above two problems, firstly, we develop a data collector and labeler, which can generate the synthetic crowd scenes and simultaneously annotate them without any manpower."
ADCrowdNet: An Attention-Injective Deformable Convolutional Network for Crowd Understanding,N. Liu; Y. Long; C. Zou; Q. Niu; L. Pan; H. Wu,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953548,NO,"We have evaluated our method on four popular crowd counting datasets (ShanghaiTech, UCF_CC_50, WorldEXPO'10, and UCSD) and an extra vehicle counting dataset TRANCOS, and our approach beats existing state-of-the-art approaches on all of these datasets. With the attention-aware training scheme and multi-scale deformable convolutional scheme, the proposed ADCrowdNet achieves the capability of being more effective to capture the crowd features and more resistant to various noises."
Wide-Area Crowd Counting via Ground-Plane Density Maps and Multi-View Fusion CNNs,Q. Zhang; A. B. Chan,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953461,NO,"However, single-view counting is not applicable to large and wide scenes (e.g., public parks, long subway platforms, or event spaces) because a single camera cannot capture the whole scene in adequate detail for counting, e.g., when the scene is too large to fit into the field-of-view of the camera, too long so that the resolution is too low on faraway crowds, or when there are too many large objects that occlude large portions of the crowd. We consider 3 versions of the fusion framework: the late fusion model fuses camera-view density map; the naive early fusion model fuses camera-view feature maps; and the multi-view multi-scale early fusion model favors that features aligned to the same ground-plane point have consistent scales."
Learning to Estimate Robust 3D Human Mesh from In-the-Wild Crowded Scenes,H. Choi; G. Moon; J. Park; K. M. Lee,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879767,YES,"A motion capture dataset, which provides accurate 3D labels for training, lacks crowd data and impedes a network from learning crowded scene-robust image features of a target person. First, we leverage 2D human pose estimation that does not require a motion capture dataset with 3D labels for training and does not suffer from the domain gap."
HiVT: Hierarchical Vector Transformer for Multi-Agent Motion Prediction,Z. Zhou; L. Ye; J. Wang; K. Wu; K. Lu,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878832,NO,"Meanwhile, we propose a translation-invariant scene representation and rotation-invariant spa-tial learning modules, which extract features robust to the geometric transformations of the scene and enable the model to make accurate predictions for multiple agents in a single forward pass. How-ever, existing methods neglect the symmetries of the prob-lem and suffer from the expensive computational cost, facing the challenge of making real-time multi-agent motion prediction without sacrificing the prediction performance."
Deep Decision Trees for Discriminative Dictionary Learning with Adversarial Multi-agent Trajectories,T. Fernando; S. Sridharan; C. Fookes; S. Denman,2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575387,NO,"With the explosion in the availability of spatio-temporal tracking data in modern sports, there is an enormous opportunity to better analyse, learn and predict important events in adversarial group environments. Due to the ample volume of data available, we focus on soccer tracking data, although our approach can be used in any adversarial multi-agent domain."
Adaptive NMS: Refining Pedestrian Detection in a Crowd,S. Liu; D. Huang; Y. Wang,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953514,NO,"The contributions are threefold: (1) we propose adaptive-NMS, which applies a dynamic suppression threshold to an instance, according to the target density; (2) we design an efficient subnetwork to learn density scores, which can be conveniently embedded into both the single-stage and two-stage detectors; and (3) we achieve state of the art results on the CityPersons and CrowdHuman benchmarks. This paper addresses this problem by a novel Non-Maximum Suppression (NMS) algorithm to better refine the bounding boxes given by detectors."
MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera,F. Wimbauer; N. Yang; L. von Stumberg; N. Zeller; D. Cremers,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578522,NO,"With the model trained on KITTI, we furthermore demonstrate that MonoRec is able to generalize well to both the Oxford RobotCar dataset and the more challenging TUM-Mono dataset recorded by a handheld camera. To deal with dynamic objects in the scene, we introduce a MaskModule that predicts moving object masks by leveraging the photometric inconsistencies encoded in the cost volumes."
Recurrent Attentive Zooming for Joint Crowd Counting and Precise Localization,C. Liu; X. Weng; Y. Mu,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953396,NO,"First, our formulation is based on a crucial observation that localization tends to be inaccurate at high-density regions, and increasing the resolution is an effective albeit simple solution for improving localization. A majority of existing works adopt a methodology that first estimates a person-density map and then calculates integral over this map to obtain the final count."
Density Map Regression Guided Detection Network for RGB-D Crowd Counting and Localization,D. Lian; J. Li; J. Zheng; W. Luo; S. Gao,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954179,NO,"Specifically, to improve the robustness of detection-based approaches for small/tiny heads, we leverage density map to improve the head/non-head classification in detection network where density map serves as the probability of a pixel being a head. Further, our method can be readily extended to RGB image based crowd counting and achieves comparable performance on the ShanghaiTech Part\_B dataset for both counting and localization."
Boosting Crowd Counting via Multifaceted Attention,H. Lin; Z. Ma; R. Ji; Y. Wang; X. Hong,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880163,YES,"As large-scale variations often exist within crowd images, neither fixed-size convolution kernel of CNN nor fixed-size attention of recent vision transformers can well handle this kind of variations. Secondly, we design the Local Attention Regularization to supervise the training of LRA by minimizing the deviation among the attention for different feature locations."
TrafficSim: Learning to Simulate Realistic Multi-Agent Behaviors,S. Suo; S. Regalado; S. Casas; R. Urtasun,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577667,NO,"Existing simulation environments rely on heuristic-based models that directly encode traffic rules, which cannot capture irregular maneuvers (e.g., nudging, U-turns) and complex interactions (e.g., yielding, merging). To learn a robust policy amenable for long horizon simulation, we unroll the policy in training and optimize through the fully differentiable simulation across time."
Multi-Agent Automated Machine Learning,Z. Wang; K. Su; J. Zhang; H. Jia; Q. Ye; X. Xie; Z. Lu,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205095,NO,"MA2ML takes each machine learning module, such as data augmentation (AUG), neural architecture search (NAS), or hyper-parameters (HPO), as an agent and the final performance as the reward, to formulate a multi-agent reinforcement learning problem. In this paper, we propose multi-agent automated machine learning (MA2ML) with the aim to effectively handle joint optimization of modules in automated machine learning (AutoML)."
Diverse Generation for Multi-Agent Sports Games,R. A. Yeh; A. G. Schwing; J. Huang; K. Murphy,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953867,NO,"We also measure the distribution of statistics of interest, such as player location or velocity, and show that the distribution induced by our generative model better matches the empirical distribution of the test set. Finally, we show that our model can perform conditional prediction, which lets us answer counterfactual questions such as “how will the players move differently if A passes the ball to B instead of C?”"
STCrowd: A Multimodal Dataset for Pedestrian Perception in Crowded Scenes,P. Cong; X. Zhu; F. Qiao; Y. Ren; X. Peng; Y. Hou; L. Xu; R. Yang; D. Manocha; Y. Ma,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880203,YES,"In addition, considering the property of sparse global distribution and density-varying local distribution of pedestrians, we further propose a novel method, Density-aware Hierarchical heatmap Aggregation (DHA), to enhance pedestrian perception in crowded scenes. However, existing benchmarks either only provide 2D annotations, or have limited 3D annotations with low-density pedestrian distribution, making it difficult to build a reliable pedestrian perception system especially in crowded scenes."
Optimal Transport Minimization: Crowd Localization on Density Maps for Semi-Supervised Counting,W. Lin; A. B. Chan,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205094,YES,"However, most methods do not further explore the ability to localize people in the density map, with those few works adopting simple methods, like finding the local peaks in the density map. The objective of OT-M is to find a target point map that has the minimal Sinkhorn distance with the input density map, and we propose an iterative algorithm to compute the solution."
Cross-View Cross-Scene Multi-View Crowd Counting,Q. Zhang; W. Lin; A. B. Chan,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577502,NO,"To dynamically handle the challenge of optimal view fusion under scene and camera layout change and non-correspondence noise due to camera calibration errors or erroneous features, we propose a CVCS model that attentively selects and fuses multiple views together using camera layout geometry, and a noise view regularization method to train the model to handle non-correspondence errors. We also generate a large synthetic multi-camera crowd counting dataset with a large number of scenes and camera views to capture many possible variations, which avoids the difficulty of collecting and annotating such a large real dataset."
Bi-level Alignment for Cross-Domain Crowd Counting,S. Gong; S. Zhang; J. Yang; D. Dai; B. Schiele,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878759,YES,"To reduce the domain gap between the synthetic and real data, we design a bi-level alignment framework (BLA) consisting of (1) task-driven data alignment and (2) fine-grained feature alignment. To avoid reliance on such annotations, previous works apply unsupervised domain adaptation (UDA) techniques by transferring knowledge learned from easily accessible synthetic data to real-world datasets."
Towards Professional Level Crowd Annotation of Expert Domain Data,P. Wang; N. Vasconcelos,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204827,NO,"To enable annotation by non-experts, classes are specified implicitly, via positive and negative sets of examples and augmented with deliberative explanations, which highlight regions of class ambiguity. It is a human-in-the-loop SSL method, where crowd-source workers act as filters of pseudo-labels, replacing the unreliable confidence thresholding used by state-of-the-art SSL methods."
LaPred: Lane-Aware Prediction of Multi-Modal Future Trajectories of Dynamic Agents,B. Kim; S. H. Park; S. Lee; E. Khoshimjonov; D. Kum; J. Kim; J. S. Kim; J. W. Choi,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578925,NO,"It is paramount to develop a prediction model that can exploit the contextual information in both static and dynamic environments surrounding the target agent and generate diverse trajectory samples that are meaningful in a traffic context. In this paper, we address the problem of predicting the future motion of a dynamic agent (called a target agent) given its current and past states as well as the information on its environment."
NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects,Z. Yan; C. Li; G. H. Lee,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204790,YES,"Although it warps moving points across frames from the observation spaces to a common canonical space for rendering, dynamic NeRF does not model the change of the reflected color during the warping. The experimental results demonstrate that our method significantly improves the reconstruction quality of moving specular objects from monocular RGB videos compared to the existing NeRF models."
Crowd Activity Change Point Detection in Videos via Graph Stream Mining,M. Yang; L. Rashidi; S. Rajasegarar; C. Leckie; A. S. Rao; M. Palaniswami,2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575520,NO,"In our proposed novel framework, a hyperspherical clustering algorithm is utilized for the automatic identification of interesting regions, then the density of pedestrian flows between every pair of interesting regions over consecutive time intervals is monitored and represented as a sequence of adjacency matrices where the direction and density of flows are captured through a directed graph. In this work, we address this task by proposing a novel activity change point detection method to identify crowd movement anomalies for video surveillance."
MotionDiffuser: Controllable Multi-Agent Motion Prediction Using Diffusion,C. “. Jiang; A. Cornman; C. Park; B. Sapp; Y. Zhou; D. Anguelov,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203706,NO,"Furthermore, we utilize a compressed trajectory representation via PCA, which improves model performance and allows for efficient computation of the exact sample log probability. Second, the simple predictor design requires only a single L2 loss training objective, and does not depend on trajectory anchors."
OcTr: Octree-Based Transformer for 3D Object Detection,C. Zhou; Y. Zhang; J. Chen; D. Huang,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203760,NO,"It first constructs a dynamic octree on the hierarchical feature pyramid through conducting self-attention on the top level and then recursively propagates to the level below restricted by the octants, which captures rich global context in a coarse-to-fine manner while maintaining the computational complexity under control. Albeit recent efforts made by Transformers with the long sequence modeling capability, they fail to properly balance the accuracy and efficiency, suffering from inadequate receptive fields or coarse-grained holistic correlations."
"Point in, Box Out: Beyond Counting Persons in Crowds",Y. Liu; M. Shi; Q. Zhao; X. Wang,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953871,NO,"An online updating scheme is introduced to refine the pseudo ground truth during training; while a locally-constrained regression loss is designed to provide additional constraints on the size of the predicted boxes in a local neighborhood. The detection-based methods, on the other hand, have not been largely explored in recent trends of crowd counting due to the needs for expensive bounding box annotations."
SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in Urban Environments,Y. Dai; Y. Lin; X. Lin; C. Wen; L. Xu; H. Yi; S. Shen; Y. Ma; C. Wang,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204011,NO,"Even-tually, SLOPER4D consists of 15 sequences of human motions, each of which has a trajectory length of more than 200 meters (up to 1,300 meters) and covers an area of more than 200 m2 (up to 30,000 m2), including more than 100k LiDAR frames, 300k video frames, and 500k IMU-based motion frames. To obtain accurate 3D ground truth in such large dynamic scenes, we propose a joint optimization method to fit local SMPL meshes to the scene and fine-tune the camera calibration during dynamic motions frame by frame, resulting in plausible and scene-natural 3D human poses."
EqMotion: Equivariant Multi-Agent Motion Prediction with Invariant Interaction Reasoning,C. Xu; R. T. Tan; Y. Tan; S. Chen; Y. G. Wang; X. Wang; Y. Wang,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205349,YES,"To further promote more comprehensive motion features, we propose an invariant pattern feature learning module to learn an invariant pattern feature, which cooperates with the equivariant geometric feature to enhance network expressiveness. Experimental results show that our method is not only generally applicable, but also achieves state-of-the-art prediction performances on all the four tasks, improving by 24.0/30.1/8.6/9.2%."
Residual Regression With Semantic Prior for Crowd Counting,J. Wan; W. Luo; B. Wu; A. B. Chan; W. Liu,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954128,NO,"Although recent deep learning based counting algorithms have achieved a great progress, the correlation knowledge among samples and the semantic prior have not yet been fully exploited. By incorporating such information into our network, we discover that more intrinsic characteristics can be learned by the network which thus generalizes better to unseen scenarios."
DynamicStereo: Consistent Dynamic Depth from Stereo Videos,N. Karaev; I. Rocco; B. Graham; N. Neverova; A. Vedaldi; C. Rupprecht,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203742,YES,"We also introduce Dynamic Replica, a new bench-mark dataset containing synthetic videos of people and ani-mals in scanned environments, which provides complemen-tary training and evaluation data for dynamic stereo closer to real applications than existing datasets. Training with this dataset further improves the quality of predictions of our proposed DynamicStereo as well as prior methods."
TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D Environments,Y. Sun; Q. Bao; W. Liu; T. Mei; M. J. Black,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204771,YES,"Although the estimation of 3D human pose and shape (HPS) is rapidly progressing, current methods still cannot reliably estimate moving humans in global coordinates, which is critical for many applications. To address these issues, we adopt a novel 5D representation (space, time, and identity) that enables end-to-end reasoning about people in scenes."
CrowdPose: Efficient Crowded Scenes Pose Estimation and a New Benchmark,J. Li; C. Wang; H. Zhu; Y. Mao; H. -S. Fang; C. Lu,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954341,NO,"In this paper, we propose a novel and efficient method to tackle the problem of pose estimation in the crowd and a new dataset to better evaluate algorithms. With multi-peak prediction for each joint and global association using the graph model, our method is robust to inevitable interference in crowded scenes and very efficient in inference."
Crowd Counting and Density Estimation by Trellis Encoder-Decoder Networks,X. Jiang; Z. Xiao; B. Zhang; X. Zhen; X. Cao; D. Doermann; L. Shao,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954254,NO,"First, we develop a new trellis architecture that incorporates multiple decoding paths to hierarchically aggregate features at different encoding stages, which improves the representative capability of convolutional features for large variations in objects. Finally, on four widely-used benchmarks, our TEDnet achieves the best overall performance in terms of both density map quality and counting accuracy, with an improvement up to 14% in MAE metric."
Leveraging Heterogeneous Auxiliary Tasks to Assist Crowd Counting,M. Zhao; J. Zhang; C. Zhang; W. Zhang,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953875,NO,"We identify three geometric/semantic/numeric attributes essentially important to the density estimation, and demonstrate how to effectively utilize these heterogeneous attributes to assist the crowd counting by formulating them into multiple auxiliary tasks. With the multi-fold regularization effects induced by the auxiliary tasks, the backbone CNN model is driven to embed desired properties explicitly and thus gains robust representations towards more accurate density estimation."
Crowd Counting in the Frequency Domain,W. Shu; J. Wan; K. C. Tan; S. Kwong; A. B. Chan,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879332,NO,"We prove that our loss function is an upper bound of the pseudo sup norm metric between the ground truth and the prediction density map (over all of their sub-regions), and demonstrate its efficacy and efficiency versus other loss functions. By transforming the density map into the frequency domain and using the properties of the characteristic function, we propose a novel method that is simple, effective, and efficient."
Action4D: Online Action Recognition in the Crowd and Clutter,Q. You; H. Jiang,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954340,NO,"Our method paves the way to action recognition in the real-world applications and is ready to be deployed to enable smart homes, smart factories and smart stores. We propose to tackle this challenging problem using a holistic 4D ``scan'' of a cluttered scene to include every detail about the people and environment."
What do navigation agents learn about their environment?,K. Dwivedi; G. Roig; A. Kembhavi; R. Mottaghi,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880337,NO,"We demonstrate interesting insights about navigation agents using iSEE, including the ability to encode reachable locations (to avoid obstacles), visibility of the target, progress from the initial spawn location as well as the dramatic effect on the behaviors of agents when we mask out critical individual neurons. While past works have explored interpreting deep learning models, little attention has been devoted to interpreting embodied AI systems, which often involve reasoning about the structure of the environment, target characteristics and the outcome of one's actions."
"Detection, Tracking, and Counting Meets Drones in Crowds: A Benchmark",L. Wen; D. Du; P. Zhu; Q. Hu; Q. Wang; L. Bo; S. Lyu,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578305,NO,"To promote the developments of object detection, tracking and counting algorithms in drone-captured videos, we construct a benchmark with a new drone-captured large-scale dataset, named as DroneCrowd, formed by 112 video clips with 33, 600 HD frames in various scenarios. To exploit the context information of neighboring objects, we design the neighboring context loss to guide the association subnet training, which enforces consistent relative position of nearby objects in temporal domain."
Interpretable Social Anchors for Human Trajectory Forecasting in Crowds,P. Kothari; B. Sifringer; A. Alahi,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578212,NO,"To overcome this limitation, we leverage the power of discrete choice models to learn interpretable rule-based intents, and subsequently utilise the expressibility of neural networks to model scene-specific residual. Human trajectory forecasting in crowds, at its core, is a sequence prediction problem with specific challenges of capturing inter-sequence dependencies (social interactions) and consequently predicting socially-compliant multimodal distributions."
Ambiance in Social Media Venues: Visual Cue Interpretation by Machines and Crowds,G. Can; Y. Benkhedda; D. Gatica-Perez,2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575486,NO,"This form of visual explanation obtained from the trained ResNet-50 models were assessed by crowdworkers based on a carefully designed crowdsourcing task, in which both visual ambiance cues of venues and subjective assessment of Grad-CAM results were collected and analyzed. The results show that paintings, photos, and decorative items are strong cues for artsy ambiance, whereas type of utensils, type of lamps and presence of flowers may indicate formal ambiance."
Query-Centric Trajectory Prediction,Z. Zhou; J. Wang; Y. Li; Y. Huang,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203873,NO,"To overcome this limitation and achieve faster inference, we introduce a query-centric paradigm for scene encoding, which enables the reuse of past computations by learning representations independent of the global spacetime coordinate system. To tackle this challenge, we first employ anchor-free queries to generate trajectory proposals in a recurrent fashion, which allows the model to utilize different scene contexts when decoding waypoints at different horizons."
Large-scale Localization Datasets in Crowded Indoor Spaces,D. Lee; S. Ryu; S. Yeon; Y. Lee; D. Kim; C. Han; Y. Cabon; P. Weinzaepfel; N. Guérin; G. Csurka; M. Humenberger,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578849,NO,"They were captured in a large shopping mall and a large metro station in Seoul, South Korea, using a dedicated mapping platform consisting of 10 cameras and 2 laser scanners. In order to obtain accurate ground truth camera poses, we developed a robust LiDAR SLAM which provides initial poses that are then refined using a novel structure-from-motion based optimization."
Crowd3D: Towards Hundreds of People Reconstruction from a Single Image,H. Wen; J. Huang; H. Cui; H. Lin; Y. -K. Lai; L. Fang; K. Li,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203526,NO,"However, existing methods cannot deal with large scenes containing hundreds of people, which encounter the challenges of large number of people, large variations in human scale, and complex spatial distribution. The core of our approach is to convert the problem of complex crowd localization into pixel localization with the help of our newly defined concept, Human-scene Virtual Interaction Point (HVIP)."
Introvert: Human Trajectory Prediction via Conditional 3D Attention,N. Shafiee; T. Padir; E. Elhamifar,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577353,NO,"In this work, we propose Introvert, a model which predicts human path based on his/her observed trajectory and the dynamic scene context, captured via a conditional 3D visual attention mechanism working on the input video. Introvert infers both environment constraints and social interactions through observing the dynamic scene instead of communicating with other humans, hence, its computational cost is independent of how crowded the surrounding of a target human is."
Rawgment: Noise-Accounted RAW Augmentation Enables Recognition in a Wide Variety of Environments,M. Yoshimura; J. Otsuka; A. Irie; T. Ohashi,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204594,NO,"Unfortunately, this approach struggles to yield realistic images in terms of pixel intensity and noise distribution due to not considering the non-linearity of Image Signal Processors (ISPs) and noise characteristics of image sensors. One simple approach is to apply data augmentation such as color jitter and blur to standard RGB (sRGB) images in simple scenes."
"Pseudo-Labels for Supervised Learning on Dynamic Vision Sensor Data, Applied to Object Detection Under Ego-Motion",N. F. Y. Chen,2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575257,NO,"Using principles inspired by the retina, its high temporal resolution overcomes motion blurring, its high dynamic range overcomes extreme illumination conditions and its low power consumption makes it ideal for embedded systems on platforms such as drones and self-driving cars. We show, for the first time, event-based car detection under ego-motion in a real environment at 100 frames per second with a test average precision of 40.3% relative to our annotated ground truth."
Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion,T. Gu; G. Chen; J. Li; C. Lin; Y. Rao; J. Zhou; J. Lu,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878855,YES,"In this paper, we present a new framework to formulate the trajectory prediction task as a reverse process of motion indeterminacy diffusion (MID), in which we progressively discard indeterminacy from all the walkable areas until reaching the desired trajectory. Unlike existing stochastic trajectory prediction methods which usually use a latent variable to represent multi-modality, we explicitly simulate the process of human motion variation from indeterminate to determinate."
Robust Test-Time Adaptation in Dynamic Scenarios,L. Yuan; B. Xie; S. Li,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204405,YES,"However, these attempts may fail in dynamic scenarios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distributions."
MERLOT RESERVE: Neural Script Knowledge through Vision and Language and Sound,R. Zellers; J. Lu; X. Lu; Y. Yu; Y. Zhao; M. Salehi; A. Kusupati; J. Hessel; A. Farhadi; Y. Choi,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879062,NO,"We introduce @MERLOT RESERVE, a model that represents videos jointly over time - through a new training objective that learns from audio, subtitles, and video frames. In a fully zero-shot setting, our model obtains competitive results on four video tasks, even outperforming supervised approaches on the recently proposed Situated Reasoning (STAR) benchmark."
DR.VIC: Decomposition and Reasoning for Video Individual Counting,T. Han; L. Bai; J. Gao; Q. Wang; W. Ouyang,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879966,YES,"Instead of relying on the Multiple Object Tracking (MOT) techniques, we propose to solve the problem by decomposing all pedestrians into the initial pedestrians who existed in the first frame and the new pedestrians with separate identities in each following frame. In this work, we propose to conduct the pedes-trian counting from a new perspective - Video Individual Counting (VIC), which counts the total number of individual pedestrians in the given video (a person is only counted once)."
PoseExaminer: Automated Testing of Out-of-Distribution Robustness in Human Pose and Shape Estimation,Q. Liu; A. Kortylewski; A. Yuille,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205124,YES,"To address this fundamental problem, we develop a simulator that can be controlled in a fine-grained manner using interpretable parameters to explore the manifold of images of human pose, e.g. In addition, we show that fine-tuning HPS methods by exploiting the failure modes found by PoseExaminer improve their robustness and even their performance on standard benchmarks by a significant margin."
PhyIR: Physics-based Inverse Rendering for Panoramic Indoor Images,Z. Li; L. Wang; X. Huang; C. Pan; J. Yang,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879361,NO,"In this paper, we present PhyIR, a neural inverse rendering method with a more completed SVBRDF representation and a physics-based in-network rendering layer, which can handle complex material and incorporate physical constraints by re-rendering realistic and detailed specular reflectance. Extensive experiments on both synthetic and real-world data show that the proposed method outperforms the state-of-the-arts quantitatively and qualitatively, and is able to produce photorealistic results for a number of applications such as dynamic virtual object insertion."
Two Body Problem: Collaborative Visual Task Completion,U. Jain; L. Weihs; E. Kolve; M. Rastegari; S. Lazebnik; A. Farhadi; A. G. Schwing; A. Kembhavi,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953359,NO,"Learning to collaborate in a visual environment entails learning (1) to perform the task, (2) when and what to communicate, and (3) how to act based on these communications and the perception of the visual world. In this paper we study the problem of learning to collaborate directly from pixels in AI2-THOR and demonstrate the benefits of explicit and implicit modes of communication to perform visual tasks."
Divide-and-Conquer for Lane-Aware Diverse Trajectory Prediction,S. Narayanan; R. Moslemi; F. Pittaluga; B. Liu; M. Chandraker,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577563,NO,"Our framework provides multi-agent trajectory outputs in a forward pass by capturing interactions through hypercolumn descriptors and incorporating scene information in the form of rasterized images and per-agent lane anchors. As our first contribution, we propose a novel Divide-And-Conquer (DAC) approach that acts as a better initialization technique to WTA objective, resulting in diverse outputs without any spurious modes."
Whose Track Is It Anyway? Improving Robustness to Tracking Errors with Affinity-based Trajectory Prediction,X. Weng; B. Ivanovic; K. Kitani; M. Pavone,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879091,NO,"reduces overall prediction errors by up to 57.9%, in comparison to standard prediction pipelines that use tracklets as inputs, with even more significant error reduction (up to 88.6%) if restricting the evaluation to challenging scenarios with tracking errors. Since affinity matrices contain “soft” information about the similarity and identity of detections across frames, making prediction directly from affinity matrices retains strictly more information than making prediction from the tracklets generated by data association."
Collaboration Helps Camera Overtake LiDAR in 3D Detection,Y. Hu; Y. Lu; R. Xu; W. Xie; S. Chen; Y. Wang,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203708,NO,Camera-only 3D detection provides an economical solution with a simple configuration for localizing objects in 3D space compared to LiDAR-based detection systems. The shared messages from multiple view-points disambiguate the single-agent estimated depth and complement the occluded and long-range regions in the single-agent view.
Safe Local Motion Planning with Self-Supervised Freespace Forecasting,P. Hu; A. Huang; J. Dolan; D. Held; D. Ramanan,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577522,NO,"Practical autonomy stacks adopt a semantic object-centric representation of a dynamic scene and build object detection, tracking, and prediction modules to solve forecasting. Our key intuition is that it is important to avoid straying into occupied space regardless of what is occupying it."
The Wisdom of Crowds: Temporal Progressive Attention for Early Action Prediction,A. Stergiou; D. Damen,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205036,NO,"Extensive experiments over four video datasets showcase state-of-the-art performance on the task of Early Action Prediction across a range of encoder architectures. Early action prediction deals with inferring the ongoing action from partially-observed videos, typically at the outset of the video."
“Seeing” Electric Network Frequency from Events,L. Xu; G. Hua; H. Zhang; L. Yu; N. Qiao,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203338,YES,"In this paper, we show that the ENF can be extracted without the above limitations from a new modality provided by the so-called event camera, a neuromorphic sensor that encodes the light intensity variations and asynchronously emits events with extremely high temporal resolution and high dynamic range. Most of the artificial lights fluctuate in response to the grid's alternating current and exhibit subtle variations in terms of both intensity and spectrum, providing the potential to estimate the Electric Network Frequency (ENF)from conventional frame-based videos."
Multiple Object Tracking with Correlation Learning,Q. Wang; Y. Zheng; P. Pan; Y. Xu,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578419,NO,"To incorporate the spatial layout, we propose to exploit the local correlation module to model the topological relationship between targets and their surrounding environment, which can enhance the discriminative power of our model in crowded scenes. To exploit the temporal context, existing approaches generally utilize two or more adjacent frames to construct an enhanced feature representation, but the dynamic motion scene is inherently difficult to depict via CNNs."
ChiTransformer: Towards Reliable Stereo from Cues,Q. Su; S. Ji,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879564,YES,"To address these issues in both scenarios, we present an optic-chiasm-inspired self-supervised binocular depth estimation method, wherein vision transformer (ViT) with a gated positional cross-attention (GPCA) layer is designed to enable feature-sensitive pattern retrieval between views, while retaining the extensive context information aggregated through self-attentions. While single image depth estimation is spared from these challenges and can achieve satisfactory results with the extracted monocular cues, the lack of stereoscopic relationship renders the monocular prediction less reliable on its own especially in highly dynamic or cluttered environments."
Tracking Through Containers and Occluders in the Wild,B. Van Hoorick; P. Tokmakov; S. Stent; J. Li; C. Vondrick,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204496,NO,"We evaluate two recent transformer-based video models and find that while they can be surprisingly capable of tracking targets under certain settings of task variation, there remains a considerable performance gap before we can claim a tracking model to have acquired a true notion of object permanence. To study this task, we create a mixture of synthetic and annotated real datasets to support both supervised learning and structured evaluation of model performance under various forms of task variation, such as moving or nested containment."
Non-Probability Sampling Network for Stochastic Human Trajectory Prediction,I. Bae; J. -H. Park; H. -G. Jeon,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879462,YES,"For this purpose, we propose the Non-Probability Sampling Network (NPSN), a very small network (~5K parameters) that generates purposive sample sequences using the past paths of pedestrians and their social interactions. Through this analysis, we observe that the inferences of all stochastic models are biased toward the random sampling, and fail to generate a set of realistic paths from finite samples."
End-to-End Trajectory Distribution Prediction Based on Occupancy Grid Maps,K. Guo; W. Liu; J. Pan,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879534,NO,"Most recent works focus on predicting diverse trajectories in order to cover all modes of the real distribution, but they may despise the precision and thus give too much credit to unrealistic predictions. Yet, it is a challenging task because the ground-truth distribution is unknown and unobservable, while only one of its samples can be applied for supervising model learning, which is prone to bias."
Rethinking Efficient Lane Detection via Curve Modeling,Z. Feng; S. Guo; X. Tan; K. Xu; M. Wang; L. Ma,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879543,YES,"Unlike state-of-the-art segmentation-based and point detection-based methods that typically require heuristics to either decode predictions or formulate a large sum of anchors, the curve-based methods can learn holistic lane representations naturally. To handle the optimization difficulties of existing poly-nomial curve methods, we propose to exploit the parametric Bézier curve due to its ease of computation, stability, and high freedom degrees of transformations."
Consistent Direct Time-of-Flight Video Depth Super-Resolution,Z. Sun; W. Ye; J. Xiong; G. Choe; J. Wang; S. Su; R. Ranjan,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205312,YES,"To evaluate our models on complex dynamic indoor environments and to provide a large-scale dToF sensor dataset, we introduce Dy-DToF, the first synthetic RGB-dToF video dataset that features dynamic objects and a realistic dToF simulator following the physical imaging process. Unlike the conventional RGB-guided depth enhancement approaches, which perform the fusion in a per-frame manner, we propose the first multi-frame fusion scheme to mitigate the spatial ambiguity resulting from the low-resolution dToF imaging."
DriveGAN: Towards a Controllable High-Quality Neural Simulation,S. W. Kim; J. Philion; A. Torralba; S. Fidler,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577589,NO,"While most of the contemporary simulators are hand-crafted, a scaleable way to build simulators is to use machine learning to learn how the environment behaves in response to an action, directly from data. Since DriveGAN is a fully differentiable simulator, it further allows for re-simulation of a given video sequence, offering an agent to drive through a recorded scene again, possibly taking different actions."
FLAG3D: A 3D Fitness Activity Dataset with Language Instruction,Y. Tang; J. Liu; A. Liu; B. Yang; W. Dai; Y. Rao; J. Lu; J. Zhou; X. Li,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203838,YES,"FLAG3D features the following three aspects: 1) accurate and dense 3D human pose captured from advanced MoCap system to handle the complex activity and large movement, 2) detailed and professional language instruction to describe how to perform a specific activity, 3) versatile video resources from a high-tech MoCap system, rendering software, and cost-effective smartphones in natural environments. Extensive experiments and in-depth analysis show that FLAG3D contributes great research value for various challenges, such as cross-domain human action recognition, dynamic human mesh recovery, and language-guided human action generation."
CRAVES: Controlling Robotic Arm With a Vision-Based Economic System,Y. Zuo; W. Qiu; L. Xie; F. Zhong; Y. Wang; A. L. Yuille,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953695,NO,"In this paper, we present an alternative solution, which uses a 3D model to create a large number of synthetic data, trains a vision model in this virtual domain, and applies it to real-world images after domain adaptation. We also construct a vision-based control system for task accomplishment, for which we train a reinforcement learning agent in a virtual environment and apply it to the real-world."
4D Panoptic LiDAR Segmentation,M. Aygün; A. Ošep; M. Weber; M. Maximov; C. Stachniss; J. Behley; L. Leal-Taixé,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578492,NO,"Inspired by recent advances in benchmarking of multi-object tracking, we propose to adopt a new evaluation metric that separates the semantic and point-to-instance association aspects of the task. In this paper, we propose 4D panoptic LiDAR segmentation to assign a semantic class and a temporally-consistent instance ID to a sequence of 3D points."
HCRF-Flow: Scene Flow from Point Clouds with Continuous High-order CRFs and Position-aware Flow Embedding,R. Li; G. Lin; T. He; F. Liu; C. Shen,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578049,NO,"Although significant advances have been made by deep neural networks, the performance is far from satisfactory as only per-point translational motion is considered, neglecting the constraints of the rigid motion in local regions. In addition, constraints on the rigidity of the local transformation are also added by sharing unique rigid motion parameters for all points within each local region."
Image De-raining via Continual Learning,M. Zhou; J. Xiao; Y. Chang; X. Fu; A. Liu; J. Pan; Z. -J. Zha,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577362,NO,"The proposed model is capable of achieving superior performance on both inhomogeneous and incremental datasets, and is promising for highly compact systems to gradually learn myriad regularities of the different types of rain streaks. While deep convolutional neural networks (CNNs) have achieved great success on image de-raining task, most existing methods can only learn fixed mapping rules between paired rainy/clean images on a single dataset."
Lifelong Unsupervised Domain Adaptive Person Re-identification with Coordinated Anti-forgetting and Adaptation,Z. Huang; Z. Zhang; C. Lan; W. Zeng; P. Chu; Q. You; J. Wang; Z. Liu; Z. -J. Zha,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878424,NO,"This is challenging because it requires the model to continuously adapt to unlabeled data in the target environments while alleviating catastrophic forgetting for such a fine-grained person retrieval task. Specifically, a meta-based Coordinated Data Replay strategy is proposed to replay old data and update the network with a coordinated optimization direction for both adaptation and memorization."
WildLight: In-the-wild Inverse Rendering with a Flashlight,Z. Cheng; J. Li; H. Li,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203448,YES,"The key idea is to exploit smartphone's built-in flashlight as a minimally controlled light source, and decompose image intensities into two photometric components – a static appearance corresponds to ambient flux, plus a dynamic reflection induced by the moving flashlight. Building on the success of neural light fields, we use an off-the-shelf method to capture the ambient reflections, while the flashlight component enables physically accurate photometric constraints to decouple reflectance and illumination."
SR-LSTM: State Refinement for LSTM Towards Pedestrian Trajectory Prediction,P. Zhang; W. Ouyang; P. Zhang; J. Xue; N. Zheng,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954402,NO,"In order to address this issue, we propose a data-driven state refinement module for LSTM network (SR-LSTM), which activates the utilization of the current intention of neighbors, and jointly and iteratively refines the current states of all participants in the crowd through a message passing mechanism. To effectively extract the social effect of neighbors, we further introduce a social-aware information selection mechanism consisting of an element-wise motion gate and a pedestrian-wise attention to select useful message from neighboring pedestrians."
Contextual Instance Decoupling for Robust Multi-Person Pose Estimation,D. Wang; S. Zhang,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879833,YES,"For instance, it achieves 71.3% AP on CrowdPose, outperforming the recent single-stage DEKR by 5.6%, the bottom-up CenterAttention by 3.7%, and the top-down JC-SPPE by 5.3%. Decoupling persons into different feature maps allows to isolate distractions from other persons, and explore context cues at scales larger than the bounding box size."
MotionTrack: Learning Robust Short-Term and Long-Term Motions for Multi-Object Tracking,Z. Qin; S. Zhou; L. Wang; J. Duan; G. Hua; W. Tang,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204896,YES,"In this paper, we propose a simple yet effective multi-object tracker, i.e., MotionTrack, which learns robust short-term and long-term motions in a unified framework to associate trajectories from a short to long range. For extreme occlusions, we build a novel Refind Module to learn reliable long-term motions from the target's history trajectory, which can link the interrupted trajectory with its corresponding detection."
HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR,Y. Dai; Y. Lin; C. Wen; S. Shen; L. Xu; J. Yu; Y. Ma; C. Wang,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880196,NO,"Considering that IMUs can capture human poses but always drift for long-period use, while LiDAR is stable for global localization but rough for local positions and orientations, HSC4D makes both sensors complement each other by a joint optimization and achieves promising results for long-term capture. We propose Human-centered 4D Scene Capture (HSC4D) to accurately and efficiently create a dynamic digital world, containing large-scale indoor-outdoor scenes, diverse human motions, and rich interactions between humans and environments."
SoPhie: An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints,A. Sadeghian; V. Kosaraju; A. Sadeghian; N. Hirose; H. Rezatofighi; S. Savarese,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953374,NO,"We present SoPhie; an interpretable framework based on Generative Adversarial Network (GAN), which leverages two sources of information, the path history of all the agents in a scene, and the scene context information, using images of the scene. Our approach blends a social attention mechanism with physical attention that helps the model to learn where to look in a large scene and extract the most salient parts of the image relevant to the path."
Towards Social Artificial Intelligence: Nonverbal Social Signal Prediction in a Triadic Interaction,H. Joo; T. Simon; M. Cikara; Y. Sheikh,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954115,NO,"We present a new research task and a dataset to understand human social interactions via computational methods, to ultimately endow machines with the ability to encode and decode a broad channel of social signals humans use. We then present a new 3D motion capture dataset to explore this problem, where the broad spectrum of social signals (3D body, face, and hand motions) are captured in a triadic social interaction scenario."
M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction,Q. Sun; X. Huang; J. Gu; B. C. Williams; H. Zhao,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879119,NO,"Our proposed approach M2I first classifies interacting agents as pairs of influencers and reactors, and then leverages a marginal prediction model and a conditional prediction model to predict trajectories for the influencers and reactors, respectively. Existing models excel at predicting marginal trajectories for single agents, yet it remains an open question to jointly predict scene compliant trajectories over multiple agents."
Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale,R. Ramrakhya; E. Undersander; D. Batra; A. Das,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880429,NO,"More importantly, we find the IL-trained agent learns efficient object-search behavior from humans - it peeks into rooms, checks corners for small objects, turns in place to get a panoramic view - none of these are exhibited as prominently by the RL agent, and to induce these behaviors via contemporary RL techniques would require tedious reward engineering. First, we develop a virtual teleoperation data-collection infrastructure - connecting Habitat simulator running in a web browser to Amazon Mechanical Turk, allowing remote users to teleoperate virtual robots, safely and at scale."
Transitional Adaptation of Pretrained Models for Visual Storytelling,Y. Yu; J. Chung; H. Yun; J. Kim; G. Kim,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577852,NO,"We propose a novel approach named Transitional Adaptation of Pre-trained Model (TAPM) that adapts the multi-modal modules to each other with a simpler alignment task between visual inputs only with no need for text labels. However, this direct transfer practice may suffer from the discord between visual specificity and language fluency since they are often separately trained from large corpora of visual and text data with no common ground."
Is Mapping Necessary for Realistic PointGoal Navigation?,R. Partsey; E. Wijmans; N. Yokoyama; O. Dobosevych; D. Batra; O. Maksymets,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879299,NO,"For the task of PointGoal navigation ('Go to Δx, Δy’) under idealized settings (no RGB-D and actuation noise, perfect GPS+Compass), the answer is a clear ‘yes' - mapless neural models composed of task-agnostic components (CNNs and RNNs) trained with large-scale reinforcement learning achieve 100% Success on a standard dataset (Gibson [24] ). While our approach does not saturate or ‘solve’ this dataset, this strong improvement combined with promising zero-shot sim2real transfer (to a LoCoBot robot) provides evidence consistent with the hypothesis that explicit mapping may not be necessary for navigation, even in a realistic setting."
Pushing it out of the Way: Interactive Visual Navigation,K. -H. Zeng; L. Weihs; A. Farhadi; R. Mottaghi,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578659,YES,"More specifically, we consider two downstream tasks in the physics-enabled, visually rich, AI2-THOR environment: (1) reaching a target while the path to the target is blocked (2) moving an object to a target location by pushing it. For both tasks, agents equipped with an NIE significantly outperform agents without the understanding of the effect of the actions indicating the benefits of our approach."
"Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation",S. Chen; P. -L. Guhur; M. Tapaswi; C. Schmid; I. Laptev,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879544,NO,"To balance the complexity of large action space reasoning and fine-grained language grounding, we dynamically combine a fine-scale encoding over local observations and a coarse-scale encoding on a global map via graph transformers. The agent not only needs to ground languages in visual scenes, but also should explore the environment to reach its target."
MUSE-VAE: Multi-Scale VAE for Environment-Aware Long Term Trajectory Prediction,M. Lee; S. S. Sohn; S. Moon; S. Yoon; M. Kapadia; V. Pavlovic,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880379,NO,"Accurate long-term trajectory prediction in complex scenes, where multiple agents (e.g., pedestrians or vehicles) interact with each other and the environment while attempting to accomplish diverse and often unknown goals, is a challenging stochastic forecasting problem. We demonstrate these assertions through a comprehensive set of experiments on nuScenes and SDD benchmarks as well as PFSD, a new synthetic dataset, which challenges the forecasting ability of models on complex agent-environment interaction scenarios."
Envedit: Environment Editing for Vision-and-Language Navigation,J. Li; H. Tan; M. Bansal,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879298,YES,"Empirically, on both the Room-to-Room and the multi-lingual Room-Across-Room datasets, we show that our proposed Envedit method gets significant im-provements in all metrics on both pre-trained and non-pre-trained VLN agents, and achieves the new state-of-the-art on the test leaderboard. To address this problem, we propose Envedit, a data augmentation method that cre-ates new environments by editing existing environments, which are used to train a more generalizable agent."
Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation,Y. Hong; Z. Wang; Q. Wu; S. Gould,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879946,NO,"The fundamental difference between the two setups is that discrete navigation assumes prior knowledge of the connectivity graph of the environment, so that the agent can effectively transfer the problem of navigation with low-level controls to jumping from node to node with high-level actions by grounding to an image of a navigable direction. Through extensive experiments we show that agents navigating in continuous environments with predicted waypoints perform significantly better than agents using low-level actions, which reduces the absolute discrete-to-continuous gap by 11.76% Success Weighted by Path Length (SPL) for the Cross-Modal Matching Agent and 18.24% SPL for the VLN$$BERT."
Opening up Open World Tracking,Y. Liu; I. E. Zulfikar; J. Luiten; A. Dave; D. Ramanan; B. Leibe; A. Ošep; L. Leal-Taixé,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880207,NO,An autonomous agent that is blind to never-seen-before objects poses a safety hazard when operating in the real world - and yet this is how almost all current systems work. We hope to open a new front in multi-object tracking research that will hopefully bring us a step closer to intelligent systems that can operate safely in the real world.
Temporal Complementarity-Guided Reinforcement Learning for Image-to-Video Person Re-Identification,W. Wu; J. Liu; K. Zheng; Q. Sun; Z. Zha,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880409,NO,"Specifically, TCRL formulates point-to-set matching procedure as Markov decision process, where a sequential judgement agent measures the uncertainty between the query image and all historical frames at each time step, and verifies that sufficient complementary clues are accumulated for judgment (same or different) or one more frames are requested to assist judgment. Existing methods treat it as a cross-modality retrieval task and learn the common latent embeddings from image and video modalities, which are both less effective and efficient due to large modality gap and redundant feature learning by utilizing all video frames."
Consistency driven Sequential Transformers Attention Model for Partially Observable Scenes,S. B. Rangrej; C. L. Srinidhi; J. J. Clark,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880412,NO,"Furthermore, to improve classification performance, we introduce a novel training objective, which enforces consistency between the class distribution predicted by a teacher model from a complete image and the class distribution predicted by our agent using glimpses. When the agent senses only 4% of the total image area, the inclusion of the proposed consistency loss in our training objective yields 3% and 8% higher accuracy on ImageNet and fMoW datasets, respectively."
ProphNet: Efficient Agent-Centric Motion Forecasting with Anchor-Informed Proposals,X. Wang; T. Su; F. Da; X. Yang,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204544,NO,"Due to the heterogeneous nature of multi-sourced input, multimodality in agent behavior, and low latency required by onboard deployment, this task is notoriously challenging. We generate diverse proposals, fused with anchors bearing goal-oriented scene context, to induce multimodal prediction that covers a wide range of future trajectories."
Decentralized Learning with Multi-Headed Distillation,A. Zhmoginov; M. Sandler; N. Miller; G. Kristiansen; M. Vladymyrov,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203315,NO,"We study the effects of data and model architecture heterogeneity and the impact of the underlying communication graph topology on learning efficiency and show that our agents can significantly improve their performance compared to learning in isolation. We propose a novel distillation-based decentralized learning technique that allows multiple agents with private non-lid data to learn from each other, without having to share their data, weights or weight updates."
EXCALIBUR: Encouraging and Evaluating Embodied Exploration,H. Zhu; R. Kapoor; S. Y. Min; W. Han; J. Li; K. Geng; G. Neubig; Y. Bisk; A. Kembhavi; L. Weihs,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204393,NO,"EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like: “is the small heavy red bowl made from glass?” or “is there a silver spoon heavier than the egg?”. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions."
Imitation Learning as State Matching via Differentiable Physics,S. Chen; X. Ma; Z. Xu,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204050,YES,"In this work, we identify the benefits of differentiable physics simulators and propose a new IL method, i.e., Imitation Learning as State Matching via Differentiable Physics (ILD), which gets rid of the double-loop design and achieves significant improvements in final performance, convergence speed, and stability. Existing imitation learning (IL) methods such as inverse reinforcement learning (IRL) usually have a double-loop training process, alternating between learning a reward function and a policy and tend to suffer long training time and high variance."
D2Former: Jointly Learning Hierarchical Detectors and Contextual Descriptors via Agent-Based Transformers,J. He; Y. Gao; T. Zhang; Z. Zhang; F. Wu,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203099,NO,"To deal with these issues, a novel im-age matching method is proposed by Jointly Learning Hier-archical Detectors and Contextual Descriptors via Agent-based Transformers (D2Former), including a contextual feature descriptor learning (CFDL) module and a hierar-chical keypoint detector learning (HKDL) module. How-ever, achieving robust image matching remains challenging because CNN extracted descriptors usually lack discrim-inative ability in texture-less regions and keypoint detec-tors are only good at identifying keypoints with a specific level of structure."
MIXSIM: A Hierarchical Framework for Mixed Reality Traffic Simulation,S. Suo; K. Wong; J. Xu; J. Tu; A. Cui; S. Casas; R. Urtasun,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205254,NO,"Towards this goal, we propose to leverage the wealth of interesting scenarios captured in the real world and make them reactive and controllable to enable closed-loop SDV evaluation in what-if situations. Furthermore, by varying each agent's route, we can expand the scope of testing to what-if situations with realistic variations in agent behaviors or even safety critical interactions."
Behavioral Analysis of Vision-and-Language Navigation Agents,Z. Yang; A. Majumdar; S. Lee,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203467,NO,"In this work, we develop a methodology to study agent behavior on a skill-specific basis - examining how well existing agents ground instructions about stopping, turning, and moving towards specified objects or rooms. We present a detailed case study analyzing the behavior of a recent agent and then compare multiple agents in terms of skill-specific competency scores."
Coaching a Teachable Student,J. Zhang; Z. Huang; E. Ohn-Bar,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203984,NO,"Our proposed sensorimotor agent results in a robust image-based behavior cloning agent in CARLA, improving over current models by over 20.6% in driving score without requiring LiDAR, historical observations, ensemble of models, on-policy data aggregation or reinforcement learning. Current distillation for sensorimotor agents methods tend to result in suboptimal learned driving behavior by the student, which we hypothesize is due to inherent differences between the input, modeling capacity, and optimization processes of the two agents."
Towards real-world navigation with deep differentiable planners,S. Ishida; J. F. Henriques,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878917,YES,"To avoid the potentially hazardous trial-and-error of reinforcement learning, we focus on differentiable planners such as Value Iteration Networks (VIN), which are trained offline from safe expert demonstrations. As far as we know, we are the first to successfully apply them to the difficult Active Vision Dataset, consisting of real images captured from a robot."
GATSBI: Generative Agent-centric Spatio-temporal Object Interaction,C. -H. Min; J. Bae; J. Lee; Y. M. Kim,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578878,NO,"We present GATSBI, a generative model that can transform a sequence of raw observations into a structured latent representation that fully captures the spatiotemporal context of the agent’s actions. The agent requires a good scene representation of the visual observation that discerns essential components and consistently propagates along the time horizon."
GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving,Y. Chen; F. Rong; S. Duggal; S. Wang; X. Yan; S. Manivasagam; S. Xue; E. Yumer; R. Urtasun,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578059,NO,"During simulation, we perform a novel geometry-aware simulation-by-composition procedure which 1) proposes plausible and realistic object placements into a given scene, 2) renders novel views of dynamic objects from the asset bank, and 3) composes and blends the rendered image segments. In this paper, we present GeoSim, a geometry-aware image composition process which synthesizes novel urban driving scenarios by augmenting existing images with dynamic objects extracted from other scenes and rendered at novel poses."
PifPaf: Composite Fields for Human Pose Estimation,S. Kreiss; L. Bertoni; A. Alahi,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953198,NO,"Our method outperforms previous methods at low resolution and in crowded, cluttered and occluded scenes thanks to (i) our new composite field PAF encoding fine-grained information and (ii) the choice of Laplace loss for regressions which incorporates a notion of uncertainty. The new method, PifPaf, uses a Part Intensity Field (PIF) to localize body parts and a Part Association Field (PAF) to associate body parts with each other to form full human poses."
Parsing R-CNN for Instance-Level Human Analysis,L. Yang; Q. Song; Z. Wang; M. Jiang,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953214,NO,"It processes a set of human instances simultaneously through comprehensive considering the characteristics of region-based approach and the appearance of a human, thus allowing representing the details of instances. Instance-level human analysis is common in real-life scenarios and has multiple manifestations, such as human part segmentation, dense pose estimation, human-object interactions, etc."
Deep Single Image Camera Calibration With Radial Distortion,M. López; R. Marí; P. Gargallo; Y. Kuang; J. Gonzalez-Jimenez; G. Haro,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954230,NO,We thoroughly analyze the performance of the proposed method and the impact of the improvements and compare with previous approaches for single image radial distortion correction. In this work we propose a method to predict extrinsic (tilt and roll) and intrinsic (focal length and radial distortion) parameters from a single image.
How many Observations are Enough? Knowledge Distillation for Trajectory Forecasting,A. Monti; A. Porrello; S. Calderara; P. Coscia; L. Ballan; R. Cucchiara,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879765,NO,"We feel that this common schema neglects critical traits of realistic applications: as the collection of input trajectories involves machine perception (i.e., detection and tracking), incorrect detection and fragmentation errors may accumulate in crowded scenes, leading to tracking drifts. Current state-of-the-art models usually rely on a “history” of past tracked locations (e.g., 3 to 5 seconds) to predict a plausible sequence of future locations (e.g., up to the next 5 seconds)."
Instance-wise Occlusion and Depth Orders in Natural Scenes,H. Lee; J. Park,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879265,NO,"The scenes were annotated by 3,659 crowd-workers regarding (1) occlusion order that identifies occluder/occludee and (2) depth order that describes ordinal relations that consider relative distance from the camera. Moreover, we propose a dense depth prediction network called InstaDepthNet that uses auxiliary geometric order loss to boost the accuracy of the state-of-the-art depth prediction approach, MiDaS [54]."
Privacy Preserving Localization and Mapping from Uncalibrated Cameras,M. Geppert; V. Larsson; P. Speciale; J. L. Schönberger; M. Pollefeys,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578648,NO,"Recent works on localization and mapping from privacy preserving line features have made significant progress towards addressing the privacy concerns arising from cloud-based solutions in mixed reality and robotics. This enables uncalibrated devices to both localize themselves within an existing map as well as contribute to the map, while preserving the privacy of the image contents."
Beyond Supervised vs. Unsupervised: Representative Benchmarking and Analysis of Image Representation Learning,M. Gwilliam; A. Shrivastava,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879692,NO,"We reveal through our analysis that in isolation, single popular methods should not be treated as though they represent the field as a whole, and that future work ought to consider how to leverage the complimentary nature of these methods. To enrich this comparison, we analyze embeddings with measurements such as uniformity, tolerance, and centered kernel alignment (CKA), and propose two new metrics of our own: nearest neighbor graph similarity and linear prediction overlap."
KeyTr: Keypoint Transporter for 3D Reconstruction of Deformable Objects in Videos,D. Novotny; I. Rocco; S. Sinha; A. Carlier; G. Kerchenbaum; R. Shapovalov; N. Smetanin; N. Neverova; B. Graham; A. Vedaldi,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879736,NO,"Compared to weaker deformation models, this significantly reduces the reconstruction ambiguity and, for dynamic objects, allows Keypoint Transporter to obtain reconstructions of the quality superior or at least comparable to prior approaches while being much faster and reliant on a pre-trained monocular depth estimator network. The idea is to fit a dynamic point cloud to the video data using Sinkhorn's algorithm to associate the 3D points to 2D pixels and use a differentiable point renderer to ensure the compatibility of the 3D deformations with the measured optical flow."
Trajectory-Aware Body Interaction Transformer for Multi-Person Pose Forecasting,X. Peng; S. Mao; Z. Wu,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203728,NO,"On both short-and long-term horizons, we empirically evaluate our frame-work on CMU-Mocap, MuPoTS-3D as well as synthesized datasets (6 ~ 10 persons), and demonstrate that our method greatly outperforms the state-of-the-art methods. Specifically, we construct a Temporal Body Partition Module that transforms all the pose sequences into a Multi-Person Body-Part sequence to retain spatial and temporal information based on body semantics."
MoDi: Unconditional Motion Synthesis from Diverse Data,S. Raab; I. Leibovitch; P. Li; K. Aberman; O. Sorkine-Hornung; D. Cohen-Or,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205193,YES,"Despite the lack of any structure in the dataset, our model yields a well-behaved and highly structured latent space, which can be semantically clustered, constituting a strong motion prior that facilitates various applications including semantic editing and crowd animation. In addition, we present an encoder that inverts real motions into MoDi's natural motion manifold, issuing solutions to various ill-posed challenges such as completion from prefix and spatial editing."
Box-Level Active Detection,M. Lyu; J. Zhou; H. Chen; Y. Huang; D. Yu; Y. Li; Y. Guo; Y. Guo; L. Xiang; G. Ding,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203997,YES,"It exploits both human annotations and the model intelligence in a complementary fashion: an efficient input-end committee queries labels for informative objects only; meantime well-learned targets are identified by the model and compensated with pseudo-labels. Having revealed above problems and limitations, we introduce a box-level active detection framework that controls a box-based budget per cycle, prioritizes informative targets and avoids redundancy for fair comparison and efficient application."
Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable Categories,S. Sinha; R. Shapovalov; J. Reizenstein; I. Rocco; N. Neverova; A. Vedaldi; D. Novotny,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204777,YES,"At test time, given a small number of video frames of an unseen object, Tracker-NeRF predicts the trajectories of its 3D points and generates new views, interpolating viewpoint and time. We use cats and dogs as a representative example and introduce Common Pets in 3D (CoP3D), a collection of crowd-sourced videos of approximately 4,200 distinct pets."
Mutual Information-Based Temporal Difference Learning for Human Pose Estimation in Video,R. Feng; Y. Gao; X. Ma; T. H. E. Tse; H. J. Chang,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204825,NO,"We further propose a Representation Disentanglement module from the mutual information perspective, which can grasp discriminative task-relevant motion signals by explicitly defining useful and noisy constituents of the raw motion features and minimizing their mutual information. In this paper, we present a novel multi-frame human pose estimation framework, which employs temporal differences across frames to model dynamic contexts and engages mutual information objectively to facilitate useful motion information disentanglement."
Learning to Adapt for Stereo,A. Tonioni; O. Rahnama; T. Joy; L. Di Stefano; T. Ajanthan; P. H. S. Torr,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954176,NO,"Even though deep learning based stereo methods are successful, they often fail to generalize to unseen variations in the environment, making them less suitable for practical applications such as autonomous driving. Specifically, our approach incorporates the adaptation procedure into the learning objective to obtain a base set of parameters that are better suited for unsupervised online adaptation."
Meta Agent Teaming Active Learning for Pose Estimation,J. Gong; Z. Fan; Q. Ke; H. Rahmani; J. Liu,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880091,NO,"Finally, we show experimental results on both human hand and body pose estimation benchmark datasets and demonstrate that our method significantly outperforms all baselines continuously under the same amount of annotation budget. Our MATAL formulates the image selection procedure as a Markov Decision Process and learns an optimal sampling policy that directly maximizes the performance of the pose estimator based on the reward."
Seeing in Extra Darkness Using a Deep-Red Flash,J. Xiong; J. Wang; W. Heidrich; S. Nayar,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578549,NO,"Our main observation is that in a dim environment, the human eye mainly uses rods for the perception of light, which are not sensitive to wavelengths longer than 620nm, yet the camera sensor still has a spectral response. We propose a novel modulation strategy when training a modern CNN model for guided image filtering, fusing a noisy RGB frame and a flash frame."
SIGNet: Semantic Instance Aided Unsupervised 3D Geometry Perception,Y. Meng; Y. Lu; A. Raj; S. Sunarjo; R. Guo; T. Javidi; G. Bansal; D. Bharadia,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953332,YES,"Recent works on unsupervised learning have made considerable progress on perceiving geometry; however, they usually ignore the coherence of objects and perform poorly under scenarios with dark and noisy environments. Specifically, SIGNet integrates semantic information to make depth and flow predictions consistent with objects and robust to low lighting conditions."
Neuro-Inspired Eye Tracking With Eye Movement Dynamics,K. Wang; H. Su; Q. Ji,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953735,NO,"Combined with the bottom-up gaze measurements from the deep convolutional neural network, our method achieves better performance for both within-dataset and cross-dataset evaluations compared to state-of-the-art. In particular, we propose a novel Dynamic Gaze Transition Network (DGTN) to capture the underlying eye movement dynamics and serve as the topdown gaze prior."
Neural Cellular Automata Manifold,A. H. Ruiz; A. Vilalta; F. Moreno-Noguer,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578885,NO,"In biological terms, our approach would play the role of the transcription factors, modulating the mapping of genes into specific proteins that drive cellular differentiation, which occurs right before the morphogenesis. We accomplish this by introducing dynamic convolutions inside an Auto-Encoder architecture, for the first time used to join two different sources of information, the encoding and cell’s environment information."
Dense Distinct Query for End-to-End Object Detection,S. Zhang; X. Wang; J. Wang; J. Pang; C. Lyu; W. Zhang; P. Luo; K. Chen,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203166,YES,"However, it triggers a new dilemma as the widely used sparse queries cannot guarantee a high recall, while dense queries inevitably bring more similar queries and encounter optimization difficulties. Most impressively, DDQ-DETR achieves 52.1 AP on MS-COCO dataset within 12 epochs using a ResNet-50 backbone, outperforming all existing detectors in the same setting."
Real-time Object Detection for Streaming Perception,J. Yang; S. Liu; Z. Li; X. Li; J. Sun,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879790,YES,"In this paper, instead of searching trade-offs between accuracy and speed like previous works, we point out that endowing real-time models with the ability to predict the future is the key to dealing with this problem. While past works ignore the inevitable changes in the environment after processing, streaming perception is proposed to jointly evaluate the latency and accuracy into a single metric for video online perception."
FlowNet3D: Learning Scene Flow in 3D Point Clouds,X. Liu; C. R. Qi; L. J. Guibas,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953876,NO,"Our network simultaneously learns deep hierarchical features of point clouds and flow embeddings that represent point motions, supported by two newly proposed learning layers for point sets. Many applications in robotics and human-computer interaction can benefit from understanding 3D motion of points in a dynamic environment, widely noted as scene flow."
Self-Supervised Pillar Motion Learning for Autonomous Driving,C. Luo; X. Yang; A. Yuille,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578855,NO,"In this paper, we seek to answer the research question of whether the abundant unlabeled data collections can be utilized for accurate and efficient motion learning. To this end, we propose a learning framework that leverages free supervisory signals from point clouds and paired camera images to estimate motion purely via self-supervision."
Class-Incremental Exemplar Compression for Class-Incremental Learning,Z. Luo; Y. Liu; B. Schiele; Q. Sun,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203819,YES,"We propose an adaptive mask generation model called class-incremental masking (CIM) to explicitly resolve two difficulties of using CAM: 1) transforming the heatmaps of CAM to 0–1 masks with an arbitrary threshold leads to a trade-off between the coverage on discriminative pixels and the quantity of exemplars, as the total memory is fixed; and 2) optimal thresholds vary for different object classes, which is particularly obvious in the dynamic environment of CIL. We conduct extensive experiments on high-resolution CIL benchmarks including Food-101, ImageNet-100, and ImageNet-1000, and show that using the compressed exemplars by CIM can achieve a new state-of-the-art CIL accuracy, e.g., 4.8 percentage points higher than FOSTER [42] on 10-Phase ImageNet-1000."
Weakly Supervised Class-agnostic Motion Prediction for Autonomous Driving,R. Li; H. Shi; Z. Fu; Z. Wang; G. Lin,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205104,NO,"To this end, we propose a two-stage weakly supervised approach, where the segmentation model trained with the incomplete binary masks in Stage1 will facilitate the self-supervised learning of the motion prediction network in Stage2 by estimating possible moving foregrounds in advance. Based on this observation, we study a novel weakly supervised motion prediction paradigm, where fully or partially (1 %, 0.1%) annotated foreground/background binary masks are used for supervision, rather than using expensive motion annotations."
Event-Guided Person Re-Identification via Sparse-Dense Complementary Learning,C. Cao; X. Fu; H. Liu; Y. Huang; K. Wang; J. Luo; Z. -J. Zha,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203615,YES,"Specifically, for frames, we build a CNN-based module to aggregate the dense features of pedestrian appearance step by step, while for event streams, we design a bio-inspired spiking neural network (SNN) backbone, which encodes event signals into sparse feature maps in a spiking form, to extract the dynamic motion cues of pedestrians. In this work, we propose a Sparse-Dense Complementary Learning (SDCL) Framework, which effectively extracts identity features by fully exploiting the complementary information of dense frames and sparse events."
OTA: Optimal Transport Assignment for Object Detection,Z. Ge; S. Liu; Z. Li; O. Yoshie; J. Sun,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577660,YES,"In this paper, we innovatively revisit the label assignment from a global perspective and propose to formulate the assigning procedure as an Optimal Transport (OT) problem – a well-studied topic in Optimization Theory. After formulation, finding the best assignment solution is converted to solve the optimal transport plan at minimal transportation costs, which can be solved via Sinkhorn-Knopp Iteration."
Diverse Part Discovery: Occluded Person Re-identification with Part-Aware Transformer,Y. Li; J. He; T. Zhang; X. Liu; Y. Zhang; F. Wu,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578649,NO,"To address these issues, we propose a novel end-to-end Part-Aware Transformer (PAT) for occluded person Re-ID through diverse part discovery via a transformer encoder-decoder architecture, including a pixel context based transformer encoder and a part prototype based transformer decoder. First, to the best of our knowledge, this is the first work to exploit the transformer encoder-decoder architecture for occluded person Re-ID in a unified deep model."
Improving Multiple Pedestrian Tracking by Track Management and Occlusion Handling,D. Stadler; J. Beyerer,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578417,NO,"In contrast, we propose a novel occlusion handling strategy that explicitly models the relation between occluding and occluded tracks outperforming the feature-based approach, while not depending on a separate re-identification network. Furthermore, we improve the track management of a regression-based method in order to bypass missing detections and to deal with tracks leaving the scene at the border of the image."
An Aggregated Multicolumn Dilated Convolution Network for Perspective-Free Counting,D. Deb; J. Ventura,2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575518,NO,"Our experiments show that our proposed network outperforms the state-of-the-art on many benchmark datasets, and also that using our aggregation module in combination with a higher number of columns is beneficial for multiscale counting. While the use of multiple columns to extract multiscale information from images has been shown before, our approach aggregates the multiscale information gathered by the multicolumn convolutional neural network to improve performance."
Learning a Facial Expression Embedding Disentangled from Identity,W. Zhang; X. Ji; K. Chen; Y. Ding; C. Fan,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577420,NO,"To reduce the optimization difficulty caused by additional fully connection layers, DLN directly provides high-order polynomial to nonlinearly project the high-dimensional feature to a low-dimensional manifold. In this paper, we model the expression as the deviation from the identity by a subtraction operation, extracting a continuous and identity-invariant expression embedding."
Learning Cross-Modal Retrieval with Noisy Labels,P. Hu; X. Peng; H. Zhu; L. Zhen; J. Lin,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577762,NO,"Besides, a simple yet effective multimodal loss function, called Multimodal Contrastive loss (MC), is proposed to maxi-mize the mutual information between different modalities, thus alleviating the interference of noisy samples and cross-modal discrepancy. To tackle the challenge, this paper presents a general Multi-modal Robust Learning framework (MRL) for learning with multimodal noisy labels to mitigate noisy samples and correlate distinct modalities simultaneously."
Rethinking Spatial Invariance of Convolutional Networks for Object Counting,Z. -Q. Cheng; Q. Dai; H. Li; J. Song; X. Wu; A. G. Hauptmann,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879478,YES,"In this paper, we try to use locally connected Gaussian kernels to replace the original convolution filter to estimate the spatial position in the density map. Our work points a new direction for follow-up research, which should investigate how to properly relax the overly strict pixel-level spatial invariance for object counting."
Occluded Human Mesh Recovery,R. Khirodkar; S. Tripathi; K. Kitani,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879237,NO,"Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHu- man datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption."
Variational Pedestrian Detection,Y. Zhang; H. He; J. Li; Y. Li; J. See; W. Lin,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578255,NO,"Pedestrian detection in a crowd is a challenging task due to a high number of mutually-occluding human instances, which brings ambiguity and optimization difficulties to the current IoU-based ground truth assignment procedure in classical object detection methods. Experiments conducted on CrowdHuman and CityPersons datasets show that the proposed algorithm serves as an efficient solution to handle the dense pedestrian detection problem for the case of single-stage detectors."
Behavior and Personality Analysis in a Nonsocial Context Dataset,D. Dotti; M. Popa; S. Asteriadis,2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575484,NO,"The majority of existing methods investigate personality assessment in social contexts, such as crowded places or social events, but ignore the role of behaviors as well as personality in nonsocial situations (i.e. Forty-six participants were recorded in an unconstrained indoor space, related to a smart home environment, performing six tasks resembling Activities of Daily Living (ADL)."
ElePose: Unsupervised 3D Human Pose Estimation by Predicting Camera Elevation and Learning Normalizing Flows on 2D Poses,B. Wandt; J. J. Little; H. Rhodin,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879908,NO,"Therefore, we propose an unsupervised approach that learns to predict a 3D human pose from a single image while only being trained with 2D pose data, which can be crowd-sourced and is already widely available. Another part of our contribution is to stabilize training with normalizing flows on high-dimensional 3D pose data by first projecting the 2D poses to a linear subspace."
LMGP: Lifted Multicut Meets Geometry Projections for Multi-Camera Multi-Object Tracking,D. M. H. Nguyen; R. Henschel; B. Rosenhahn; D. Sonntag; P. Swoboda,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879609,YES,Tracklets are then matched to multi-camera trajectories by solving a global lifted multicut formulation that incorporates short and long-range temporal interactions on tracklets located in the same camera as well as inter-camera ones. Multi-Camera Multi-Object Tracking is currently drawing attention in the computer vision field due to its superior performance in real-world applications such as video surveillance with crowded scenes or in wide spaces.
Knowledge Mining with Scene Text for Fine-Grained Recognition,H. Wang; J. Liao; T. Cheng; Z. Gao; H. Liu; B. Ren; X. Bai; W. Liu,2022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880093,YES,"However, the existing methods mainly exploit the literal meaning of scene text for fine-grained recognition, which might be irrelevant when it is not significantly related to objects/scenes. We propose an end-to-end trainable network that mines implicit contextual knowledge behind scene text image and enhance the semantics and correlation to fine-tune the image representation."
OrienterNet: Visual Localization in 2D Public Maps with Neural Matching,P. -E. Sarlin; D. DeTone; T. -Y. Yang; A. Avetisyan; J. Straub; T. Malisiewicz; S. R. Bulo; R. Newcombe; P. Kontschieder; V. Balntas,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204555,YES,"OrienterNet estimates the location and orientation of a query image by matching a neural Bird's-Eye View with open and globally available maps from OpenStreetMap, enabling anyone to localize anywhere such maps are available. We bridge this gap by introducing OrienterNet, the first deep neural network that can localize an image with sub-meter accuracy using the same 2D semantic maps that humans use."
MDQE: Mining Discriminative Query Embeddings to Segment Occluded Instances on Challenging Videos,M. Li; S. Li; W. Xiang; L. Zhang,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203297,YES,"This is mainly because instance queries in these methods cannot encode well the discriminative embeddings of instances, making the query-based segmenter difficult to distinguish those ‘hard’ instances. While impressive progress has been achieved, video instance segmentation (VIS) methods with per-clip input often fail on challenging videos with occluded objects and crowded scenes."
Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion,D. Rempe; Z. Luo; X. B. Peng; Y. Yuan; K. Kitani; K. Kreis; S. Fidler; O. Litany,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203180,NO,"We further propose utilizing the value function learned during RL training of the animation controller to guide diffusion to produce trajectories better suited for particular scenarios such as collision avoidance and traversing uneven terrain. This trajectory diffusion model is integrated with a novel physics-based humanoid controller to form a closed-loop, full-body pedestrian animation system capable of placing large crowds in a simulated environment with varying terrains."
HumanBench: Towards General Human-Centric Perception with Projector Assisted Pretraining,S. Tang; C. Chen; Q. Xie; M. Chen; Y. Wang; Y. Ci; L. Bai; F. Zhu; H. Yang; L. Yi; R. Zhao; W. Ouyang,2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204833,YES,"Specifically, we propose a HumanBench based on existing datasets to comprehensively evaluate on the common ground the generalization abilities of different pretraining methods on 19 datasets from 6 diverse downstream tasks, including person ReID, pose estimation, human parsing, pedestrian attribute recognition, pedestrian detection, and crowd counting. To learn both coarse-grained and fine-grained knowledge in human bodies, we further propose a Projector AssisTed Hierarchical pretraining method (PATH) to learn diverse knowledge at different granularity levels."
